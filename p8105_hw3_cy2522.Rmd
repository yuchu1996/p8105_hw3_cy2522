---
title: "p8105_hw3_cy2522"
author: "Chu YU"
date: "2018-Oct-6"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# problem 1
```{r}
## import the data
library(p8105.datasets)
library(tidyverse)
theme_set(theme_bw() + theme(legend.position = "right"))
data(brfss_smart2010)

## do some data cleaning
brfss_tidy = brfss_smart2010 %>% 
  janitor::clean_names() %>%
  filter(topic == "Overall Health") %>%
  mutate(response = 
           factor(response, order = TRUE, levels = c("Excellent", "Very good", "Good", "Fair","Poor"))) 
  
```

## the questions in problem 1
```{r}
# answer the questions

## which state is observed at 7 locations:
filter(brfss_tidy, year == "2002") %>% 
  group_by(locationabbr) %>%
  summarize(n_location = n_distinct(locationdesc)) %>%
  filter(n_location == 7)
  
## number of observations in each state in different years
brfss_tidy %>% 
  group_by(year, locationabbr) %>%
  summarize(n_location = n_distinct(locationdesc)) %>%
  ggplot(aes(x = year, y = n_location, color = locationabbr)) +
  geom_line() +
  labs( 
    title = "spaghetti plot of observations in each state",
    x = "year",
    y = "number of locations")

## table for NY state
NY_table = filter(brfss_tidy , 
                  response == "Excellent" & locationabbr == "NY" & (year == "2002" | year == "2006"| year == "2010")) %>%
  group_by(year) %>%
  summarize(mean(data_value), sd(data_value)) %>%
  knitr::kable(digits = 3, col.names = c("year", "mean", "sd"), caption = "the form of Excellent response data in NY") 

NY_table

## 5-panel plots for each state and each year
brfss_tidy %>% 
  group_by(year, response, locationabbr) %>%
  summarize(mean = mean(data_value)) %>%
  ggplot(aes(x = year, y = mean)) +
           geom_point(alpha = .1) +
           facet_grid(. ~ response) +
            labs( 
    title = " plot of responses in each state over the years",
    x = "year",
    y = "mean of response proportion") +
   theme(axis.text.x = element_text(angle = 60, hjust = 1))
           

```
 Problem 1 answers:
 
 **1.cleaning the dataset:**
 
 - clean the names of the variables of the brfss data.
    
 - `filter` the topic so that there is only one topic "Overall Health"
    
 - use `mutate` function so that I can set the order of the factor response, from which I can also see delete the other seponses in the dataset.

 **2. questions:**

**In 2002, which states were observed at 7 locations?**

  - In 2002, **CT, FL, NC** were observed at 7 locations. 
    (After `group_by` and `summarize` we can get the number of observations of each state, then we can get the three states whose observations equal 7.)
    
**Make a spaghetti plot that shows the number of locations in each state from 2002 to 2010.**

  - We can see the plot named "spaghetti plot of observations in each state from 2002 to 2010" above. And we can also see the different tendency of observations of each state and the differences between them.
  
  - "FL" varies steeply from 2006 to 2010.
  
**Make a table showing, for the years 2002, 2006, and 2010, the mean and standard deviation of the proportion of Excellent responses across locations in NY State.**    

  - From the table shown above, from 2002 to 2010, the sd is getting smaller, meaning that the response value data of Excellent is getting more stable.

**a five-panel plot that shows**   

  - In the plot we can get the different mean proportions of each state, x represents the different years, and y represents the mean proportions. We can know the distribution of responses in the states and the overall situation of responses proportions nationwide.

# problem 2
```{r}
## import the data
library(p8105.datasets)

data(instacart)

## exploration of the data
inst_tidy = instacart %>%
  janitor::clean_names()

 dim(inst_tidy)
 str(inst_tidy)
 n_distinct(inst_tidy$product_id) 
```
 problem 2
 
 **1. Exploration of the datasets**

   - From the datasets, we can see the size is  1384617, 15, meaning there are 1384614 observations of 15 variables. Using `str()` we can get the overall struction of the data.
   
   - The variables in the dataset are characters and integers. The variable order_id , product_id, user_id , add_to_cart_order, reordered, order_number, order_dow , order_hour_of_day, days_since_prior_order, aisle_id and department_id are all integers, and there are character variables like eval_set, product_name, department and aisle.  
   There are key variables like order_number, aisle, department and order_id.
   
   - We can know the meaning of some variables via the values and names of them -- the IDs of the products, users, departments and orders, the names of the products, the number of orders and so on. So from the data we can know that this may be the trading data from instacart.
   
   For order id 1, we can see this customer whose user id is 112108 added 8 products to cart and then reordered some of them. The day of the order since prior order was 9 days.Then we can also see the products' names and the aisles and departments they are from.
   
   By using `n_distinct`, we can get the exact number of the orders and products and other things . For example, we can know there are 39123 products sold in the dataset.
   
 **2. questions**
```{r, fig.height = 15, fig.width = 8}
## How many aisles are there, and which aisles are the most items ordered from?

n_distinct(inst_tidy$aisle)
names(table(inst_tidy$aisle))[which.max(table(inst_tidy$aisle))]

## Make a plot that shows the number of items ordered in each aisle. Order aisles sensibly, and organize your plot so others can read it.

inst_tidy %>%
  group_by(aisle) %>%
  summarise(n = n()) %>%
  ggplot(aes( y = n,x = reorder(aisle, n))) +
  geom_histogram(stat = "identity") +
  coord_flip() +
  labs( 
    title = " plot of aisles data",
    x = "aisle",
    y = "number")

## Make a table showing the most popular item aisles 

inst_tidy %>%
  filter(aisle %in% c("baking ingredients", "dog food care", "packaged vegetables fruits")) %>%
  group_by(aisle) %>%
  summarize(
    mostpop = names(table(product_name))[which.max(table(product_name))]) %>%
  knitr::kable(caption = "table of most popular items")
  
## Make a table showing the mean hour of the day
 
inst_tidy %>% 
  filter(product_name %in% c("Pink Lady Apples", "Coffee Ice Cream")) %>%
  group_by(product_name, order_dow) %>%
  summarize(mean = mean(order_hour_of_day)) %>%
  spread(key = order_dow, value = mean) %>%
  knitr::kable(digits = 3, caption = "the table of mean hour")
  
  
```
 
problem 2

 **2. questions:**
 
 **(1) How many aisles are there, and which aisles are the most items ordered from?**
 
- There are 134 distinct aisles, and most items ordered from "fresh vegetables".

**(2) Make a plot that shows the number of items ordered in each aisle. **

- From the plot above, we can see the different number of products ordered in each aisle. Histogram can clearly present the differences between the amounts of each variable. And to be clearer, I use colors depend on the department. But the plot still need to be seen from a bigger window due to the large number of aisles.

**(3) Make a table showing the most popular item from certain aisles.**

- The 3*2 table has two variables -- "aisle" and "mostpop". From the table,  we can know that:

  - In "baking ingredients" aisle, "Light Brown Sugar" is the most popular;
  
  - In "dog food care", "Snacks sticks Chicken & Rice Recipe Dog Treats" is the most popular;
  
  - In "packages vegetables fruits", "Organic Baby Spinach" is the most popular.
  
**(4) Make a table showing the mean hour of the day:**

- I finally get a 2*7 table. There are 7 key variables as column names -- from 0 to 6, and the row names are the names of the two products. The values are means of the hours of each day in a week. 

# Problem 3
```{r}
## import the data
data(ny_noaa)

## exploration of the data

 dim(ny_noaa)
 str(ny_noaa)

```

 **1. Exploration of the datasets**

   - From the datasets, we can see the size is  2595176 obs. of  7 variables. Using `str()` we can get the overall struction of the data.
   
   - The variables in the dataset are characters,integers and dates format. But from the str() results we can see that the head of the data frame are all missing values, which is hard to read. Id, tmax, tmin are characters, meanwhile prcp, snow and snwd are integers, date is Date format. There are some key variables like date (Date, format), prcp(integer), snow(integer), tmax (character) and so on. 
   We can see the mode of every columns and know the meaning of them from the data. For example, the prcp,snow and snwd are three different weather. And there are temperatures on different dates. So we can know the data frame is a weather table of NY.
   
    - Form the data frame , we can see that almost every row of "tmax" and "tmin" is missing value. There are 1000 missing values in both tmax and tmin. This is definitely an issue. I think when all values in a row are missing, it must impact others' understanding of the row. When someone can not tell what is a column or a row for from the values, the missing value is an issue and should be cleared.
    
## problem 3 questions
```{r}
## Do some data cleaning. Create separate variables for year, month, and day.
noaa_tidy = ny_noaa %>% 
  janitor::clean_names() %>%
  separate(date, into = c("year", "month", "day"), sep = "-") %>%
  mutate(tmax = 0.1*as.numeric(tmax) , tmin = 0.1*as.numeric(tmin) )
  

## for snowfall
names(table(noaa_tidy$snow))[which.max(table(noaa_tidy$snow))]

## Make a two-panel plot showing the average temperature in January and in July in each station across years.

noaa_tidy %>%
  filter(month %in% c("01", "07")) %>%
  group_by(month, id, year) %>%
  summarize(average_tmax = mean(tmax, na.rm = T)) %>%
  ggplot(aes( x = year, y = average_tmax, fill = month)) +
  geom_boxplot() +
  facet_grid(month ~ .) +
  theme(axis.text.x = element_text(angle = 60, hjust = 1)) +
  labs( 
    title = " plot of average tmax",
    x = "year",
    y = "average tmax")

  
  
## Make a two-panel plot showing (i) tmax vs tmin for the full dataset  
library(hexbin)
library(patchwork)

panel_1_temp = ggplot(noaa_tidy, aes(x = tmin, y = tmax)) +
  geom_hex()

panel_2_sonw = noaa_tidy %>% 
  filter(snow > 0 & snow < 100) %>%
  ggplot( aes(x = factor(year), y = snow)) +
  geom_boxplot() +
  theme(axis.text.x = element_text(angle = 60, hjust = 1))

panel_1_temp + panel_2_sonw
```
 **prblem 3 - questions**

 - To give a reasonable units, I change the units of temperature to "degree centigrade".
   For snowfall, "0" is the most commonly observed value. It may because that New York only snows in winter, which is not that long.


 **Is there any observable / interpretable structure? Any outliers?**  

 - From the boxplot above, we can see that the tmax of Janury fluctuate mostly in interval of (-3, 5), and there are outliers on both sides and the plots skew to the higher temperature. Compared with Janury, July has higher tmax average, around 27, and there are more outliers of lower tmperature. The plots mostly skew to lower temperature.
 
 
 **Make a two-panel plot **

 -  We can see from the first plot the relationship of tmax and tmin. And there are few extreme temperatures.
    From the second plot, we can see the boxplots skew to the higher snowfall.
    